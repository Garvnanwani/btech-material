{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ac2338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words and Word Count:\n",
      "Word: this\tCount: 1\n",
      "Word: is\tCount: 1\n",
      "Word: a\tCount: 1\n",
      "Word: sample\tCount: 1\n",
      "Word: text\tCount: 1\n",
      "Word: corpus\tCount: 3\n",
      "Word: it\tCount: 3\n",
      "Word: contains\tCount: 1\n",
      "Word: multiple\tCount: 1\n",
      "Word: sentences\tCount: 4\n",
      "Word: some\tCount: 1\n",
      "Word: may\tCount: 1\n",
      "Word: repeat\tCount: 1\n",
      "Word: while\tCount: 1\n",
      "Word: others\tCount: 1\n",
      "Word: are\tCount: 1\n",
      "Word: unique\tCount: 2\n",
      "Word: the\tCount: 7\n",
      "Word: program\tCount: 1\n",
      "Word: should\tCount: 3\n",
      "Word: tokenize\tCount: 2\n",
      "Word: into\tCount: 2\n",
      "Word: words\tCount: 2\n",
      "Word: and\tCount: 2\n",
      "Word: print\tCount: 1\n",
      "Word: along\tCount: 1\n",
      "Word: with\tCount: 1\n",
      "Word: their\tCount: 1\n",
      "Word: counts\tCount: 1\n",
      "Word: also\tCount: 1\n",
      "Word: calculate\tCount: 1\n",
      "Word: size\tCount: 1\n",
      "Word: of\tCount: 1\n",
      "Word: vocabulary\tCount: 1\n",
      "Word: by\tCount: 1\n",
      "Word: removing\tCount: 1\n",
      "Word: duplicates\tCount: 1\n",
      "Word: finally\tCount: 1\n",
      "Word: remove\tCount: 1\n",
      "Word: duplicate\tCount: 1\n",
      "Word: assign\tCount: 1\n",
      "Word: ids\tCount: 1\n",
      "Word: to\tCount: 1\n",
      "Word: each\tCount: 1\n",
      "Word: sentence\tCount: 1\n",
      "Word: display\tCount: 1\n",
      "Word: document\tCount: 1\n",
      "Vocabulary Size: 47\n",
      "\n",
      "Sentences and Sentence Count:\n",
      "Sentence 1: \n",
      "This is a sample text corpus.\n",
      "Sentence 2: It contains multiple sentences.\n",
      "Sentence 3: Some sentences may repeat, while others are unique.\n",
      "Sentence 4: \n",
      "The program should tokenize the corpus into words and print the words along with their counts.\n",
      "Sentence 5: It should also calculate the size of the vocabulary by removing duplicates.\n",
      "Sentence 6: Finally, it should tokenize the corpus into sentences, remove duplicate sentences, assign unique IDs to each sentence, and display the document.\n",
      "Sentence 7: \n",
      "\n",
      "Unique Sentences:\n",
      "ID: 1\tSentence: This is a sample text corpus.\n",
      "ID: 2\tSentence: It contains multiple sentences.\n",
      "ID: 3\tSentence: Some sentences may repeat, while others are unique.\n",
      "ID: 4\tSentence: The program should tokenize the corpus into words and print the words along with their counts.\n",
      "ID: 5\tSentence: It should also calculate the size of the vocabulary by removing duplicates.\n",
      "ID: 6\tSentence: Finally, it should tokenize the corpus into sentences, remove duplicate sentences, assign unique IDs to each sentence, and display the document.\n",
      "ID: 7\tSentence: \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def tokenize_words(corpus):\n",
    "    # Tokenize the corpus into words\n",
    "    words = re.findall(r'\\b\\w+\\b', corpus.lower())\n",
    "    return words\n",
    "\n",
    "\n",
    "def calculate_word_count(words):\n",
    "    # Calculate word count\n",
    "    word_count = {}\n",
    "    for word in words:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "    return word_count\n",
    "\n",
    "\n",
    "def calculate_vocabulary_size(words):\n",
    "    # Calculate size of vocabulary\n",
    "    vocabulary = set(words)\n",
    "    return len(vocabulary)\n",
    "\n",
    "\n",
    "def tokenize_sentences(corpus):\n",
    "    # Tokenize the corpus into sentences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', corpus)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def remove_duplicate_sentences(sentences):\n",
    "    # Remove duplicate sentences and assign unique ids\n",
    "    unique_sentences = {}\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        sentence = sentence.strip()\n",
    "        if sentence not in unique_sentences:\n",
    "            unique_sentences[sentence] = index + 1\n",
    "    return unique_sentences\n",
    "\n",
    "\n",
    "def display_document(sentences):\n",
    "    # Display the document with unique sentence ids\n",
    "    for sentence, sentence_id in sentences.items():\n",
    "        print(f\"ID: {sentence_id}\\tSentence: {sentence}\")\n",
    "\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = '''\n",
    "This is a sample text corpus. It contains multiple sentences. Some sentences may repeat, while others are unique. \n",
    "The program should tokenize the corpus into words and print the words along with their counts.\n",
    "It should also calculate the size of the vocabulary by removing duplicates.\n",
    "Finally, it should tokenize the corpus into sentences, remove duplicate sentences, assign unique IDs to each sentence, and display the document.\n",
    "'''\n",
    "\n",
    "# Step 1: Tokenize the corpus into words and print the words and word count\n",
    "words = tokenize_words(corpus)\n",
    "word_count = calculate_word_count(words)\n",
    "print(\"Words and Word Count:\")\n",
    "for word, count in word_count.items():\n",
    "    print(f\"Word: {word}\\tCount: {count}\")\n",
    "\n",
    "# Step 2: Calculate the size of the vocabulary\n",
    "vocabulary_size = calculate_vocabulary_size(words)\n",
    "print(f\"Vocabulary Size: {vocabulary_size}\")\n",
    "\n",
    "# Step 3: Tokenize the corpus into sentences and print the sentences and sentence count\n",
    "sentences = tokenize_sentences(corpus)\n",
    "print(\"\\nSentences and Sentence Count:\")\n",
    "for index, sentence in enumerate(sentences):\n",
    "    print(f\"Sentence {index+1}: {sentence}\")\n",
    "\n",
    "# Step 4: Prepare a new document by removing duplicate sentences, assigning unique ids, and display the document\n",
    "unique_sentences = remove_duplicate_sentences(sentences)\n",
    "print(\"\\nUnique Sentences:\")\n",
    "display_document(unique_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05343b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
