{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1e2730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training Corpus Tokens:\n",
      "['Ċ', 'T', 'h', 'is', 'Ġ', 'is', 'Ġa', 'Ġs', 'a', 'm', 'ple', 'Ġtraining', 'Ġcorpus', '.', 'Ġ', 'I', 't', 'Ġc', 'on', 't', 'ain', 's', 'Ġ', 'm', 'u', 'l', 't', 'i', 'ple', 'Ġs', 'en', 't', 'en', 'c', 'e', 's', 'Ġ', 'f', 'or', 'Ġtraining', 'Ġthe', 'ĠBPE', 'Ġt', 'o', 'k', 'en', 'i', 'z', 'e', 'r', '.', 'Ċ', 'T', 'he', 'ĠBPE', 'Ġa', 'l', 'g', 'or', 'it', 'h', 'm', 'Ġ', 'w', 'i', 'l', 'l', 'Ġ', 'le', 'a', 'r', 'n', 'Ġthe', 'Ġs', 'u', 'b', 'w', 'or', 'd', 'Ġ', 'u', 'n', 'it', 's', 'Ġ', 'b', 'a', 's', 'e', 'd', 'Ġ', 'on', 'Ġthe', 'Ġtraining', 'Ġcorpus', '.', 'Ċ']\n",
      "\n",
      "Test Corpus Tokens:\n",
      "['Ċ', 'T', 'h', 'is', 'Ġ', 'is', 'Ġa', 'Ġs', 'a', 'm', 'ple', 'Ġt', 'e', 's', 't', 'Ġcorpus', '.', 'Ġ', 'I', 't', 'Ġ', 'w', 'i', 'l', 'l', 'Ġ', 'b', 'e', 'Ġt', 'o', 'k', 'en', 'i', 'z', 'e', 'd', 'Ġ', 'u', 's', 'ing', 'Ġthe', 'Ġtrain', 'e', 'd', 'ĠBPE', 'Ġt', 'o', 'k', 'en', 'i', 'z', 'e', 'r', '.', 'Ċ', 'W', 'e', 'Ġc', 'a', 'n', 'Ġ', 'e', 'v', 'a', 'l', 'u', 'a', 't', 'e', 'Ġthe', 'Ġ', 'e', 'f', 'f', 'e', 'c', 't', 'i', 'v', 'en', 'e', 's', 's', 'Ġ', 'o', 'f', 'Ġthe', 'Ġt', 'o', 'k', 'en', 'i', 'z', 'e', 'r', 'Ġ', 'b', 'y', 'Ġ', 'e', 'x', 'a', 'm', 'in', 'ing', 'Ġthe', 'Ġ', 'r', 'e', 's', 'u', 'l', 't', 'ing', 'Ġt', 'o', 'k', 'en', 's', '.', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "def train_bpe_tokenizer(corpus_file, vocab_size):\n",
    "    # Initialize and train the BPE tokenizer\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train(files=[corpus_file], vocab_size=vocab_size, min_frequency=2)\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_with_bpe(tokenizer, text):\n",
    "    # Tokenize text using the trained BPE tokenizer\n",
    "    encoded = tokenizer.encode(text)\n",
    "    tokens = encoded.tokens\n",
    "    return tokens\n",
    "\n",
    "# Training Corpus\n",
    "training_corpus = '''\n",
    "This is a sample training corpus. It contains multiple sentences for training the BPE tokenizer.\n",
    "The BPE algorithm will learn the subword units based on the training corpus.\n",
    "'''\n",
    "\n",
    "# Test Corpus\n",
    "test_corpus = '''\n",
    "This is a sample test corpus. It will be tokenized using the trained BPE tokenizer.\n",
    "We can evaluate the effectiveness of the tokenizer by examining the resulting tokens.\n",
    "'''\n",
    "\n",
    "# Train BPE tokenizer\n",
    "tokenizer = train_bpe_tokenizer(corpus_file='training_corpus.txt', vocab_size=1000)\n",
    "\n",
    "# Tokenize Training Corpus\n",
    "training_tokens = tokenize_with_bpe(tokenizer, training_corpus)\n",
    "print(\"Training Corpus Tokens:\")\n",
    "print(training_tokens)\n",
    "\n",
    "# Tokenize Test Corpus\n",
    "test_tokens = tokenize_with_bpe(tokenizer, test_corpus)\n",
    "print(\"\\nTest Corpus Tokens:\")\n",
    "print(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9342c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-macosx_12_0_arm64.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a494ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
